{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RpxoYrfWNbDI"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-986488099123>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from numpy import array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpvdtyO4PN_s"
   },
   "outputs": [],
   "source": [
    "#Building vocabulary and converting each statement into vector\n",
    "\n",
    "def create_unique(text_arr):\n",
    "  dictt = {}\n",
    "  count = -1\n",
    "  x = np.array(text_arr)\n",
    "  \n",
    "  text = np.unique(x)\n",
    "  #print(text)\n",
    "  for item in text:\n",
    "    data = (item.lower().strip().split(\" \"))\n",
    "    for item1 in data:\n",
    "      if item1 not in dictt:\n",
    "        count = count +1\n",
    "        dictt[item1] = count\n",
    "  \n",
    "  #print(dictt)  \n",
    "  return dictt\n",
    "\n",
    "\n",
    "def vectorize(text_arr, dictt):\n",
    "  vectors = []\n",
    "  #print(text_arr)\n",
    "  for sentence in text_arr:\n",
    "    vector = [0] * len(dictt)\n",
    "    words = (sentence.lower().strip().split(\" \"))\n",
    "    for word in words:\n",
    "      index = dictt[word]\n",
    "      vector[index] += 1\n",
    "    \n",
    "    vectors.append(vector)\n",
    "    #print(sentence)\n",
    "  #print(array(vectors))\n",
    "  return vectors\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "# text = [\"Hello, my name is XYZ, and your name is ABC. Nice to meet you.\", \"Lol, what is your name?\"]\n",
    "# dictt = create_unique(text)\n",
    "# vector = vectorize(text, dictt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1s8Nk063drJe"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "  \n",
    "  #initializing object\n",
    "  def __init__(self, lr, num_iter, threshold = 0.5):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.threshold = threshold\n",
    "  \n",
    "  #adding X0 to X matrix, feature matrix\n",
    "  def __add_intercept(self, X):\n",
    "    ones = np.ones((X.shape[0],1))\n",
    "    print(ones.shape)\n",
    "    print(X.shape)\n",
    "    return np.concatenate((ones,X), axis=1)\n",
    "  \n",
    "  #defining sigmoid funcion\n",
    "  def _sigmoid(self,X):\n",
    "    return (1/(1+np.exp(-X)))\n",
    "  \n",
    "  #defining loss function\n",
    "  def __loss(self, h, y):\n",
    "    first_term = -y*np.log(h)\n",
    "    second_term = -(1-y)*np.log(1-h)\n",
    "    return (first_term + second_term).mean()\n",
    "  \n",
    "  #training and deriving the theta vector\n",
    "  def _train(self,X,y):\n",
    "    X = self.__add_intercept(X)\n",
    "    \n",
    "    #Initializing theta vector as zeroes\n",
    "    self.theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    #making model equation and doing gradient descent\n",
    "    for i in range(self.num_iter):\n",
    "      z = np.dot(X,self.theta)\n",
    "      h = self._sigmoid(z)\n",
    "      \n",
    "      gradient = np.dot(X.T,(h-y))/y.size\n",
    "      self.theta -= self.lr*gradient\n",
    "      \n",
    "      z = np.dot(X,self.theta)\n",
    "      h = self._sigmoid(z)\n",
    "      loss = self.__loss(h,y)\n",
    "      \n",
    "      if i%50==0:\n",
    "        print(\"iteration: \",i,\"\\t cost: \",loss)\n",
    "      \n",
    "    file = open(\"spam_model.pickle\", \"wb\")\n",
    "    pickle.dump(self.theta, file)\n",
    "    file.close()\n",
    "    \n",
    "    return True\n",
    "        \n",
    "  def predict(self, X):\n",
    "    \n",
    "    file = open('spam_model.pickle','rb')\n",
    "    self.theta = pickle.load(file)\n",
    "    \n",
    "    X = self.__add_intercept(X)\n",
    "    pred = self._sigmoid((np.dot(X,self.theta)))\n",
    "    \n",
    "    return pred>=self.threshold\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "colab_type": "code",
    "id": "XGJg8n7IjrWG",
    "outputId": "1aafacfe-bd85-462a-c50a-16cffee28068"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2b5a1c8d8532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('emails.csv')\n",
    "\n",
    "dicct = create_unique(data.text.values.tolist())\n",
    "x = np.array(vectorize(data.text.values.tolist(),dicct))\n",
    "\n",
    "file = open(\"dictionary.pickle\", \"wb\")\n",
    "pickle.dump(dicct, file)\n",
    "file.close()\n",
    "\n",
    "# x = np.array((data.text.values))\n",
    "y = np.array((data.spam.values))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "o0n4187SoFfN",
    "outputId": "689efc33-62d6-419e-a49e-d4ce184a6885"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4582, 1)\n",
      "(4582, 37442)\n",
      "iteration:  0 \t cost:  1.142278544422348\n",
      "iteration:  50 \t cost:  0.30119856374860193\n",
      "iteration:  100 \t cost:  0.2521418929351193\n",
      "(1146, 1)\n",
      "(1146, 37442)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9075043630017452"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = LogisticRegression(lr=0.01, num_iter=101)\n",
    "log._train(x_train,y_train)\n",
    "\n",
    "pr = log.predict(x_test)\n",
    "accuracy_score(pr,y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sample_logistic.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
